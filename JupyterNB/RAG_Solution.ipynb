{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Implementing Document Loaders in LangChain\n",
    "\n",
    "## Objective:\n",
    "Write a Python script that uses LangChain’s document loaders to load documents from a directory. Your task is to implement functionality that reads `.txt` and `.pdf` files and outputs their content as LangChain `Document` objects.\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements:\n",
    "1. Use LangChain’s `TextLoader` for `.txt` files and `PyPDFLoader` for `.pdf` files.\n",
    "2. Implement a function `load_documents(directory: str)` that:\n",
    "   - Iterates through all files in the specified directory.\n",
    "   - Loads the content of `.txt` and `.pdf` files.\n",
    "   - Returns a list of `Document` objects, where:\n",
    "     - **Page Content:** Contains the file’s text content.\n",
    "     - **Metadata:** Includes the filename or other relevant file metadata.\n",
    "3. Handle unsupported file types or errors gracefully.\n",
    "\n",
    "---\n",
    "\n",
    "## Input:\n",
    "- A directory containing files with extensions `.txt` and `.pdf`.\n",
    "\n",
    "---\n",
    "\n",
    "## Output:\n",
    "- A list of LangChain `Document` objects. Each document should contain:\n",
    "  - The text content of the file.\n",
    "  - Metadata such as the filename.\n",
    "\n",
    "---\n",
    "\n",
    "## Example:\n",
    "### Input:\n",
    "A directory with the following files:\n",
    "- `example.txt` containing \"Hello, this is a text file.\"\n",
    "- `example.pdf` containing \"This is a PDF document.\"\n",
    "- `image.jpg` (unsupported file type).\n",
    "\n",
    "### Output:\n",
    "```python\n",
    "[\n",
    "    Document(page_content=\"Hello, this is a text file.\", metadata={\"filename\": \"example.txt\"}),\n",
    "    Document(page_content=\"This is a PDF document.\", metadata={\"filename\": \"example.pdf\"})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "def load_documents(directory: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Skeleton Function: Load .txt and .pdf documents from a directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing files.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of LangChain Document objects.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the documents\n",
    "    documents = []\n",
    "\n",
    "    # Loop through files in the specified directory\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        try:\n",
    "            # Placeholder for loading .txt files\n",
    "            if filename.endswith(\".txt\"):\n",
    "                loader=TextLoader(file_path)\n",
    "                loaded_docs = loader.load()\n",
    "                documents.extend(loaded_docs)\n",
    "                pass  # Replace with code for TextLoader\n",
    "\n",
    "            # Placeholder for loading .pdf files\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                loaded_docs = loader.load()\n",
    "                documents.extend(loaded_docs)\n",
    "                pass  # Replace with code for PyPDFLoader\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print error for files that could not be loaded\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "    # Return the list of documents\n",
    "    return documents\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the directory path\n",
    "    directory_path = \"File path\"\n",
    "\n",
    "    # Call the function to load documents\n",
    "    docs = load_documents(directory_path)\n",
    "\n",
    "    print(len(docs))\n",
    "    \n",
    "\n",
    "    # Iterate through the loaded documents and print metadata and content preview\n",
    "    for doc in docs:\n",
    "        #print(f\"File: {doc.metadata.get('filename', 'Unknown')}, Content Preview: {doc.page_content[:100]}\")\n",
    "        print('Document(Metadata of the file is : ' + str(doc.metadata) , end = ' ')\n",
    "        print('Content is :' + doc.page_content[:200] + ')')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Chunking Data and Converting It to Vector Embeddings\n",
    "\n",
    "## Objective:\n",
    "Write a Python script that uses LangChain to:\n",
    "1. Load `.txt` and `.pdf` files as `Document` objects from a directory.\n",
    "2. Chunk the data into smaller pieces for efficient processing.\n",
    "3. Convert the chunks into vector embeddings using a text embedding model.\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements:\n",
    "1. **Document Loading**:\n",
    "   - Use LangChain’s `TextLoader` for `.txt` files and `PyPDFLoader` for `.pdf` files.\n",
    "   - Implement a function `load_documents(directory: str)` to load all files from a directory as LangChain `Document` objects.\n",
    "\n",
    "2. **Chunking**:\n",
    "   - Use LangChain’s `RecursiveCharacterTextSplitter` to split the document text into smaller chunks.\n",
    "   - Implement a function `chunk_documents(documents: List[Document]) -> List[Document]`.\n",
    "\n",
    "3. **Embedding Generation**:\n",
    "   - Use a pre-trained embedding model (e.g., `OpenAIEmbeddings` or any other LangChain-compatible embedding model).\n",
    "   - Implement a function `generate_embeddings(chunks: List[Document]) -> List[List[float]]` that converts each chunk into a vector embedding.\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - Handle unsupported file types and errors gracefully.\n",
    "\n",
    "---\n",
    "\n",
    "## Input:\n",
    "- A directory containing `.txt` and `.pdf` files.\n",
    "\n",
    "---\n",
    "\n",
    "## Output:\n",
    "- A list of vector embeddings for the chunks of the loaded documents.\n",
    "\n",
    "---\n",
    "\n",
    "## Example:\n",
    "### Input:\n",
    "A directory with the following files:\n",
    "- `example.txt` containing \"This is an example text file.\"\n",
    "- `example.pdf` containing \"This is an example PDF document.\"\n",
    "\n",
    "### Output:\n",
    "A list of embeddings (e.g., 768-dimensional vectors) for the chunks generated from the documents.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "\n",
    "\n",
    "def load_documents(directory: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Skeleton Function: Load .txt and .pdf documents from a directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing files.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of LangChain Document objects.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the documents\n",
    "    documents = []\n",
    "\n",
    "    # Loop through files in the specified directory\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        try:\n",
    "            # Placeholder for loading .txt files\n",
    "            if filename.endswith(\".txt\"):\n",
    "                loader=TextLoader(file_path)\n",
    "                loaded_docs = loader.load()\n",
    "                documents.extend(loaded_docs)\n",
    "                pass  # Replace with code for TextLoader\n",
    "\n",
    "            # Placeholder for loading .pdf files\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                loaded_docs = loader.load()\n",
    "                documents.extend(loaded_docs)\n",
    "                pass  # Replace with code for PyPDFLoader\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print error for files that could not be loaded\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "    # Return the list of documents\n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "def chunk_documents(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Splits documents into smaller chunks.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): List of LangChain Document objects.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of chunked Document objects.\n",
    "    \"\"\"\n",
    "    # Create an instance of the text splitter with specified chunk size and overlap\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "    chunks = []\n",
    "\n",
    "    # Iterate over each document and split it into chunks\n",
    "    for doc in documents:\n",
    "        # Split the document and add chunks to the list\n",
    "        chunks.extend(text_splitter.split_documents([doc]))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "def generate_embeddings(chunks: List[Document]) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Generates vector embeddings for the given chunks.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[Document]): List of chunked Document objects.\n",
    "\n",
    "    Returns:\n",
    "        List[List[float]]: A list of vector embeddings.\n",
    "    \"\"\"\n",
    "    # Initialize the OpenAI embeddings model\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        api_key=\"YOUR_API_KEY\"\n",
    "    )\n",
    "    # Generate embeddings for each chunk\n",
    "    return [embeddings.embed_query(chunk.page_content) for chunk in chunks]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample directory path where documents are stored\n",
    "    directory_path = \"File path\"\n",
    "\n",
    "    # Load documents (This function is implemented in Assignment 1) ----> Done\n",
    "    documents = load_documents(directory_path)\n",
    "\n",
    "    # Chunk the documents into smaller chunks\n",
    "    chunks = chunk_documents(documents)\n",
    "    \n",
    "\n",
    "    # Generate embeddings for the chunks\n",
    "    embeddings = generate_embeddings(chunks)\n",
    "\n",
    "    # Display first 5 embeddings for demonstration\n",
    "    for i, embedding in enumerate(embeddings[:5]):  # Display first 5 embeddings for brevity\n",
    "        print(f\"Embedding {i + 1}: {embedding[:10]}...\")  # Print first 10 dimensions for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Creating and Querying a Vector Database with Chroma\n",
    "\n",
    "## Objective:\n",
    "Write a Python script to:\n",
    "1. Create a vector database using the **FAISS** library.\n",
    "2. Store vector embeddings of document chunks in the database.\n",
    "3. Query the database using similarity search and retrieve the top `k` results.\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements:\n",
    "1. **Vector Database Creation**:\n",
    "   - Use FAISS to create a persistent vector database.\n",
    "   - Add document embeddings (e.g., from OpenAI or any other embedding model) along with metadata to the database.\n",
    "\n",
    "2. **Similarity Search**:\n",
    "   - Implement a function to query the database with a user-provided text and retrieve the top `k` most similar results.\n",
    "\n",
    "3. **Input Data**:\n",
    "   - Use a list of text chunks or embeddings for this task. You may generate these from documents (e.g., `.txt` or `.pdf` files).\n",
    "\n",
    "4. **Outputs**:\n",
    "   - Return the metadata and content of the top `k` most similar results from the database.\n",
    "\n",
    "---\n",
    "\n",
    "## Example:\n",
    "### Input:\n",
    "1. A collection of text chunks from documents such as:\n",
    "   - `\"LangChain is a framework for developing applications powered by LLMs.\"`\n",
    "   - `\"FAISS is a vector database used for storing embeddings and performing similarity search.\"`\n",
    "   - `\"Document loaders are part of LangChain and help load data from multiple formats.\"`\n",
    "\n",
    "2. Query text: `\"What is Attention?\"`\n",
    "3. `k=2`\n",
    "\n",
    "### Output:\n",
    "Top `k` results based on similarity:\n",
    "1. Content: `\"FAISS is a vector database used for storing embeddings and performing similarity search.\"`\n",
    "   Metadata: `{...}`\n",
    "2. Content: `\"LangChain is a framework for developing applications powered by LLMs.\"`\n",
    "   Metadata: `{...}`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "\n",
    "\n",
    "def load_documents(directory: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load .txt and .pdf documents from a directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing files.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of LangChain Document objects.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        try:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                loader = TextLoader(file_path)\n",
    "                loaded_docs = loader.load()\n",
    "                documents.extend(loaded_docs)\n",
    "\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                loaded_docs = loader.load()\n",
    "                documents.extend(loaded_docs)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "def initialize_faiss(documents: List[Document], db_path: str) -> FAISS:\n",
    "    \"\"\"\n",
    "    Initializes a FAISS vector database and stores documents.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): List of LangChain Document objects.\n",
    "        db_path (str): Path to store the FAISS database.\n",
    "\n",
    "    Returns:\n",
    "        FAISS: FAISS vector store object.\n",
    "    \"\"\"\n",
    "    # Initialize Ollama embeddings\n",
    "    ollama_embeddings = OllamaEmbeddings(model=\"llama2\")\n",
    "\n",
    "    # Create FAISS vector store and add documents\n",
    "    vectorstore = FAISS.from_documents(documents, ollama_embeddings)\n",
    "    \n",
    "    # Save the vector store\n",
    "    vectorstore.save_local(db_path)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def query_database(query: str, vectorstore: FAISS, k: int) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Queries the FAISS database for the top-k similar documents.\n",
    "\n",
    "    Args:\n",
    "        query (str): Query text.\n",
    "        vectorstore (FAISS): FAISS vector store object.\n",
    "        k (int): Number of top results to return.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of top-k document contents from the database.\n",
    "    \"\"\"\n",
    "    # Perform similarity search in the database\n",
    "    retrieved_documents = vectorstore.similarity_search(query, k=k)\n",
    "\n",
    "    # Extract the content from the retrieved documents\n",
    "    results = [doc.page_content for doc in retrieved_documents]\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the path to the directory where your documents are stored\n",
    "    directory_path = \"File path\"\n",
    "\n",
    "    # Load documents\n",
    "    documents = load_documents(directory_path)\n",
    "\n",
    "    # Initialize FAISS with the documents\n",
    "    db_path = \"./vector_db\"\n",
    "    vectorstore = initialize_faiss(documents, db_path)\n",
    "\n",
    "    # Load existing index (if needed)\n",
    "    vectorstore = FAISS.load_local(db_path, OllamaEmbeddings(model=\"llama2\"))\n",
    "\n",
    "    # Define the query and retrieve top-k results\n",
    "    query_text = \"What is attention?\"\n",
    "    top_k = 2\n",
    "    results = query_database(query_text, vectorstore, top_k)\n",
    "\n",
    "    # Print out the results\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"Result {i+1}: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
